# Offline AI Agent Configuration

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000

# LLM Provider
LLM_PROVIDER=ollama

# Ollama Configuration
OLLAMA_URL=http://localhost:11434
OLLAMA_DIRECT_URL=http://localhost:11434
DEFAULT_MODEL=codellama

# Hugging Face Configuration
HF_API_KEY=
HF_MODEL=meta-llama/Llama-3.1-8B-Instruct
# Leave HF_BASE_URL empty to use the hosted Hugging Face inference endpoint.
# Only set this when pointing to a custom OpenAI-compatible endpoint.
HF_BASE_URL=

# Logging
LOG_LEVEL=info

# Development
DEBUG=false
RELOAD=true

# File Operations
MAX_FILE_SIZE=10485760  # 10MB
SUPPORTED_EXTENSIONS=.py,.js,.ts,.jsx,.tsx,.java,.cpp,.c,.h,.hpp,.cs,.php,.rb,.go,.rs,.swift,.kt,.scala,.r,.m,.pl,.sh,.bash,.zsh,.fish,.ps1,.bat,.cmd,.html,.css,.scss,.sass,.less,.xml,.json,.yaml,.yml,.toml,.ini,.cfg,.conf,.env,.md,.txt,.sql,.dockerfile,.dockerignore,.gitignore

# AI Model Settings
MAX_TOKENS=2000
TEMPERATURE=0.7
TOP_P=0.9
